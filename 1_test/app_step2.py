# íŒŒì¼ëª…: app_step2.py
# 2ë‹¨ê³„: ê°ì²´ ê°ì§€ ë° ìŠ¤ì¼€ì¼ ë³´ì •

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import cv2
import numpy as np
import base64
import math
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

class SmartRoomAnalyzer:
    def __init__(self):
        self.reference_objects = {
            'door': {'width': 80, 'height': 200},
            'window': {'width': 120, 'height': 150},
            'outlet': {'width': 10, 'height': 10}
        }
    
    def analyze_image(self, image_data, reference_size=200, options=None):
        """ìŠ¤ë§ˆíŠ¸ ë¶„ì„ í•¨ìˆ˜"""
        if options is None:
            options = {'detect_windows': True, 'detect_doors': True}
            
        try:
            # ì´ë¯¸ì§€ ë””ì½”ë”©
            image_bytes = base64.b64decode(image_data.split(',')[1])
            image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)
            
            if image is None:
                raise ValueError("ì´ë¯¸ì§€ë¥¼ ë””ì½”ë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"ì´ë¯¸ì§€ í¬ê¸°: {image.shape}, ê¸°ì¤€ í¬ê¸°: {reference_size}cm")
            
            # 1ë‹¨ê³„: í–¥ìƒëœ ì „ì²˜ë¦¬
            processed_image = self.enhanced_preprocessing(image)
            
            # 2ë‹¨ê³„: ê°ì²´ ê°ì§€ (ë¬¸, ì°½ë¬¸, ì½˜ì„¼íŠ¸)
            detected_objects = self.detect_room_objects(image)
            logger.info(f"ê°ì§€ëœ ê°ì²´: {len(detected_objects)}ê°œ")
            
            # 3ë‹¨ê³„: ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ ê³„ì‚°
            smart_scale = self.calculate_smart_scale(detected_objects, reference_size, image.shape)
            logger.info(f"ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼: {smart_scale:.4f} cm/pixel")
            
            # 4ë‹¨ê³„: ê°œì„ ëœ ë°© ê²½ê³„ì„  ê°ì§€
            room_corners = self.smart_room_detection(processed_image, detected_objects)
            
            # 5ë‹¨ê³„: ì •í™•í•œ ì¹˜ìˆ˜ ê³„ì‚°
            dimensions = self.calculate_accurate_dimensions(
                room_corners, smart_scale, detected_objects
            )
            
            # 6ë‹¨ê³„: ê³ ê¸‰ ì‹œê°í™”
            result_image = self.smart_visualization(
                image, room_corners, detected_objects, dimensions
            )
            
            return {
                'success': True,
                'dimensions': dimensions,
                'detected_objects': detected_objects,
                'result_image': result_image,
                'analysis_info': {
                    'method': 'smart_cv_v2',
                    'confidence': dimensions.get('confidence', 0.0),
                    'scale_factor': smart_scale,
                    'objects_found': len(detected_objects),
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def enhanced_preprocessing(self, image):
        """í–¥ìƒëœ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        # 1. ë…¸ì´ì¦ˆ ì œê±°
        denoised = cv2.bilateralFilter(image, 9, 75, 75)
        
        # 2. ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
        gray = cv2.cvtColor(denoised, cv2.COLOR_BGR2GRAY)
        
        # 3. ì ì‘ì  íˆìŠ¤í† ê·¸ë¨ í‰í™œí™”
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)
        
        # 4. ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ë¡œ ë¯¸ì„¸ ë…¸ì´ì¦ˆ ì œê±°
        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)
        
        return blurred
    
    def detect_room_objects(self, image):
        """ë°© ë‚´ ê°ì²´ ê°ì§€ (ë¬¸, ì°½ë¬¸, ì½˜ì„¼íŠ¸)"""
        detected_objects = []
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 1. ë¬¸ ê°ì§€
        doors = self.detect_doors_advanced(gray, image)
        detected_objects.extend(doors)
        
        # 2. ì°½ë¬¸ ê°ì§€
        windows = self.detect_windows_advanced(gray, image)
        detected_objects.extend(windows)
        
        # 3. ì½˜ì„¼íŠ¸/ìŠ¤ìœ„ì¹˜ ê°ì§€
        outlets = self.detect_outlets_advanced(gray)
        detected_objects.extend(outlets)
        
        return detected_objects
    
    def detect_doors_advanced(self, gray, color_image):
        """ê³ ê¸‰ ë¬¸ ê°ì§€"""
        doors = []
        
        # ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(gray, 50, 150)
        
        # ìˆ˜ì§ì„  ê°•ì¡° (ë¬¸ì€ ë³´í†µ ì„¸ë¡œê°€ ê¸´ ì§ì‚¬ê°í˜•)
        kernel_vertical = np.array([[-1, 2, -1],
                                   [-1, 2, -1],
                                   [-1, 2, -1]], dtype=np.float32)
        vertical_edges = cv2.filter2D(edges, -1, kernel_vertical)
        
        # ì»¨íˆ¬ì–´ ì°¾ê¸°
        contours, _ = cv2.findContours(vertical_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 3000 < area < 80000:  # ë¬¸ í¬ê¸° ë²”ìœ„
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = h / w
                
                # ë¬¸ì˜ íŠ¹ì§•: ë†’ì´ê°€ ë„ˆë¹„ì˜ 2-4ë°°
                if 2.0 < aspect_ratio < 4.0:
                    # ì¶”ê°€ ê²€ì¦: ë¬¸í‹€ ìƒ‰ìƒ í™•ì¸
                    roi = color_image[y:y+h, x:x+w]
                    if self.is_door_like(roi):
                        doors.append({
                            'type': 'door',
                            'bbox': (x, y, w, h),
                            'confidence': 0.8,
                            'aspect_ratio': aspect_ratio
                        })
        
        return doors
    
    def is_door_like(self, roi):
        """ë¬¸ ê°™ì€ ì˜ì—­ì¸ì§€ í™•ì¸"""
        if roi.size == 0:
            return False
        
        # ìƒ‰ìƒ ë¶„ì„: ë¬¸ì€ ë³´í†µ ê°ˆìƒ‰, í°ìƒ‰, ë˜ëŠ” ì–´ë‘ìš´ ìƒ‰
        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
        
        # ê°ˆìƒ‰ ë²”ìœ„ (ë¬¸í‹€)
        brown_lower = np.array([10, 50, 20])
        brown_upper = np.array([20, 255, 200])
        brown_mask = cv2.inRange(hsv, brown_lower, brown_upper)
        
        # ì–´ë‘ìš´ ì˜ì—­ (ë¬¸ í‹ˆìƒˆ)
        dark_mask = cv2.inRange(hsv[:,:,2], 0, 50)
        
        total_pixels = roi.shape[0] * roi.shape[1]
        door_like_pixels = cv2.countNonZero(brown_mask) + cv2.countNonZero(dark_mask)
        
        return (door_like_pixels / total_pixels) > 0.1
    
    def detect_windows_advanced(self, gray, color_image):
        """ê³ ê¸‰ ì°½ë¬¸ ê°ì§€"""
        windows = []
        
        # ë°ì€ ì˜ì—­ ê°ì§€ (ì°½ë¬¸ì€ ìì—°ê´‘ìœ¼ë¡œ ë°ìŒ)
        _, bright_mask = cv2.threshold(gray, 160, 255, cv2.THRESH_BINARY)
        
        # ëª¨í´ë¡œì§€ ì—°ì‚°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
        bright_mask = cv2.morphologyEx(bright_mask, cv2.MORPH_CLOSE, kernel)
        bright_mask = cv2.morphologyEx(bright_mask, cv2.MORPH_OPEN, kernel)
        
        contours, _ = cv2.findContours(bright_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 8000 < area < 150000:  # ì°½ë¬¸ í¬ê¸° ë²”ìœ„
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / h
                
                # ì°½ë¬¸ì˜ íŠ¹ì§•: ê°€ë¡œê°€ ì„¸ë¡œì™€ ë¹„ìŠ·í•˜ê±°ë‚˜ ë” ê¸´ ì§ì‚¬ê°í˜•
                if 0.7 < aspect_ratio < 3.0:
                    # ì¶”ê°€ ê²€ì¦: ì°½ë¬¸ í”„ë ˆì„ í™•ì¸
                    if self.is_window_like(color_image[y:y+h, x:x+w], gray[y:y+h, x:x+w]):
                        windows.append({
                            'type': 'window',
                            'bbox': (x, y, w, h),
                            'confidence': 0.7,
                            'aspect_ratio': aspect_ratio
                        })
        
        return windows
    
    def is_window_like(self, color_roi, gray_roi):
        """ì°½ë¬¸ ê°™ì€ ì˜ì—­ì¸ì§€ í™•ì¸"""
        if color_roi.size == 0:
            return False
        
        # ë°ê¸° ë¶„ì„
        mean_brightness = np.mean(gray_roi)
        
        # ì°½ë¬¸ì€ ë³´í†µ ë°ê³ , í”„ë ˆì„ì´ ìˆìŒ
        if mean_brightness > 120:
            # ì—£ì§€ ë°€ë„ í™•ì¸ (ì°½ë¬¸ í”„ë ˆì„)
            edges = cv2.Canny(gray_roi, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            
            return edge_density > 0.05  # ì ë‹¹í•œ ì—£ì§€ê°€ ìˆì–´ì•¼ í•¨
        
        return False
    
    def detect_outlets_advanced(self, gray):
        """ê³ ê¸‰ ì½˜ì„¼íŠ¸/ìŠ¤ìœ„ì¹˜ ê°ì§€"""
        outlets = []
        
        # ì‘ì€ ì§ì‚¬ê°í˜• ê°ì§€
        edges = cv2.Canny(gray, 100, 200)
        
        # ì‘ì€ ì»¤ë„ë¡œ ì‘ì€ ê°ì²´ ê°•ì¡°
        kernel = np.ones((2, 2), np.uint8)
        edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)
        
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 100 < area < 3000:  # ì½˜ì„¼íŠ¸ í¬ê¸° ë²”ìœ„
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / h
                
                # ì½˜ì„¼íŠ¸: ì •ì‚¬ê°í˜•ì— ê°€ê¹Œìš´ í˜•íƒœ
                if 0.5 < aspect_ratio < 2.0:
                    outlets.append({
                        'type': 'outlet',
                        'bbox': (x, y, w, h),
                        'confidence': 0.5,
                        'aspect_ratio': aspect_ratio
                    })
        
        return outlets
    
    def calculate_smart_scale(self, detected_objects, reference_size, image_shape):
        """ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ ê³„ì‚°"""
        h, w = image_shape[:2]
        
        # ê°ì²´ ê¸°ë°˜ ìŠ¤ì¼€ì¼ ê³„ì‚°
        for obj in detected_objects:
            obj_type = obj['type']
            bbox = obj['bbox']
            x, y, width, height = bbox
            
            if obj_type == 'door' and obj_type in self.reference_objects:
                # ë¬¸ì˜ ë†’ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ê³„ì‚°
                real_height = self.reference_objects['door']['height']
                pixel_height = height
                scale = real_height / pixel_height
                logger.info(f"ë¬¸ ê¸°ë°˜ ìŠ¤ì¼€ì¼ ê³„ì‚°: {scale:.4f} cm/pixel")
                return scale
                
            elif obj_type == 'window' and obj_type in self.reference_objects:
                # ì°½ë¬¸ì˜ ë„ˆë¹„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ê³„ì‚°
                real_width = self.reference_objects['window']['width']
                pixel_width = width
                scale = real_width / pixel_width
                logger.info(f"ì°½ë¬¸ ê¸°ë°˜ ìŠ¤ì¼€ì¼ ê³„ì‚°: {scale:.4f} cm/pixel")
                return scale
        
        # ê°ì²´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì´ë¯¸ì§€ í¬ê¸° ê¸°ë°˜ ì¶”ì •
        # ì¼ë°˜ì ì¸ ë°© í¬ê¸°ë¥¼ ê°€ì • (3m x 4m)
        estimated_room_width = 400  # cm
        scale = estimated_room_width / w
        logger.info(f"ì´ë¯¸ì§€ í¬ê¸° ê¸°ë°˜ ìŠ¤ì¼€ì¼ ì¶”ì •: {scale:.4f} cm/pixel")
        
        return scale
    
    def smart_room_detection(self, processed_image, detected_objects):
        """ìŠ¤ë§ˆíŠ¸ ë°© ê²½ê³„ì„  ê°ì§€"""
        # ê°ì²´ë¥¼ ì œì™¸í•œ ì˜ì—­ì—ì„œ ë²½ë©´ ê°ì§€
        mask = self.create_object_mask(processed_image.shape, detected_objects)
        
        # ë§ˆìŠ¤í¬ ì ìš©ëœ ì—£ì§€ ê²€ì¶œ
        edges = cv2.Canny(processed_image, 50, 150)
        edges = cv2.bitwise_and(edges, mask)
        
        # ê°•í™”ëœ ì§ì„  ê²€ì¶œ
        lines = cv2.HoughLinesP(
            edges,
            rho=1,
            theta=np.pi/180,
            threshold=60,
            minLineLength=80,
            maxLineGap=40
        )
        
        if lines is None:
            return self.fallback_corners(processed_image.shape)
        
        # ë°© ê²½ê³„ì„  ì¶”ì¶œ
        room_corners = self.extract_room_boundaries_smart(lines, processed_image.shape)
        
        return room_corners
    
    def create_object_mask(self, image_shape, detected_objects):
        """ê°ì²´ ì˜ì—­ì„ ì œì™¸í•œ ë§ˆìŠ¤í¬ ìƒì„±"""
        h, w = image_shape[:2]
        mask = np.ones((h, w), dtype=np.uint8) * 255
        
        # ê°ì§€ëœ ê°ì²´ ì˜ì—­ì„ ë§ˆìŠ¤í¬ì—ì„œ ì œì™¸
        for obj in detected_objects:
            x, y, width, height = obj['bbox']
            # ê°ì²´ ì£¼ë³€ì— ì—¬ë°± ì¶”ê°€
            margin = 10
            x1 = max(0, x - margin)
            y1 = max(0, y - margin)
            x2 = min(w, x + width + margin)
            y2 = min(h, y + height + margin)
            
            mask[y1:y2, x1:x2] = 0
        
        return mask
    
    def extract_room_boundaries_smart(self, lines, image_shape):
        """ìŠ¤ë§ˆíŠ¸ ë°© ê²½ê³„ì„  ì¶”ì¶œ"""
        h, w = image_shape[:2]
        
        # ì§ì„ ì„ ìˆ˜í‰/ìˆ˜ì§ìœ¼ë¡œ ë¶„ë¥˜
        horizontal_lines = []
        vertical_lines = []
        
        for line in lines:
            x1, y1, x2, y2 = line[0]
            angle = math.degrees(math.atan2(y2 - y1, x2 - x1))
            length = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)
            
            if length > 60:  # ìµœì†Œ ê¸¸ì´ ì¡°ê±´
                if abs(angle) < 15 or abs(angle) > 165:
                    horizontal_lines.append(line[0])
                elif 75 < abs(angle) < 105:
                    vertical_lines.append(line[0])
        
        # ê²½ê³„ì„  ì°¾ê¸° (ê°€ì¥ìë¦¬ ìš°ì„ )
        corners = []
        
        if len(horizontal_lines) >= 2 and len(vertical_lines) >= 2:
            # ìƒí•˜ì¢Œìš° ê²½ê³„ì„  ì„ íƒ
            top_line = self.select_boundary_line(horizontal_lines, 'top', h)
            bottom_line = self.select_boundary_line(horizontal_lines, 'bottom', h)
            left_line = self.select_boundary_line(vertical_lines, 'left', w)
            right_line = self.select_boundary_line(vertical_lines, 'right', w)
            
            # êµì  ê³„ì‚°
            if all(line is not None for line in [top_line, bottom_line, left_line, right_line]):
                tl = self.line_intersection(top_line, left_line)
                tr = self.line_intersection(top_line, right_line)
                br = self.line_intersection(bottom_line, right_line)
                bl = self.line_intersection(bottom_line, left_line)
                
                valid_corners = [c for c in [tl, tr, br, bl] if c and self.is_valid_corner(c, w, h)]
                
                if len(valid_corners) >= 4:
                    corners = valid_corners[:4]
        
        if len(corners) < 4:
            corners = self.fallback_corners(image_shape)
        
        return corners
    
    def select_boundary_line(self, lines, position, dimension):
        """ê²½ê³„ì„  ì„ íƒ"""
        if not lines:
            return None
        
        if position == 'top':
            return min(lines, key=lambda l: (l[1] + l[3]) / 2)
        elif position == 'bottom':
            return max(lines, key=lambda l: (l[1] + l[3]) / 2)
        elif position == 'left':
            return min(lines, key=lambda l: (l[0] + l[2]) / 2)
        elif position == 'right':
            return max(lines, key=lambda l: (l[0] + l[2]) / 2)
        
        return lines[0]
    
    def line_intersection(self, line1, line2):
        """ë‘ ì§ì„ ì˜ êµì  ê³„ì‚°"""
        x1, y1, x2, y2 = line1
        x3, y3, x4, y4 = line2
        
        denom = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
        if abs(denom) < 1e-10:
            return None
        
        t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denom
        
        x = x1 + t * (x2 - x1)
        y = y1 + t * (y2 - y1)
        
        return (int(x), int(y))
    
    def is_valid_corner(self, corner, width, height):
        """ìœ íš¨í•œ ëª¨ì„œë¦¬ì ì¸ì§€ í™•ì¸"""
        x, y = corner
        margin = 0.05  # 5% ì—¬ë°±
        
        return (width * margin <= x <= width * (1 - margin) and 
                height * margin <= y <= height * (1 - margin))
    
    def fallback_corners(self, image_shape):
        """ë°±ì—… ëª¨ì„œë¦¬ì """
        h, w = image_shape[:2]
        margin = min(w, h) * 0.1
        
        return [
            (int(margin), int(margin)),
            (int(w - margin), int(margin)),
            (int(w - margin), int(h - margin)),
            (int(margin), int(h - margin))
        ]
    
    def calculate_accurate_dimensions(self, corners, scale_factor, detected_objects):
        """ì •í™•í•œ ì¹˜ìˆ˜ ê³„ì‚°"""
        if len(corners) < 4:
            return {
                'width': 0, 'height': 0, 'area': 0, 'perimeter': 0, 'confidence': 0.0
            }
        
        # í”½ì…€ ê±°ë¦¬ ê³„ì‚°
        tl, tr, br, bl = corners[:4]
        
        width_pixels = math.sqrt((tr[0] - tl[0])**2 + (tr[1] - tl[1])**2)
        height_pixels = math.sqrt((bl[0] - tl[0])**2 + (bl[1] - tl[1])**2)
        
        # ì‹¤ì œ í¬ê¸° ê³„ì‚°
        width_cm = width_pixels * scale_factor
        height_cm = height_pixels * scale_factor
        area_m2 = (width_cm * height_cm) / 10000
        perimeter_m = (width_cm + height_cm) * 2 / 100
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self.calculate_confidence_v2(corners, detected_objects, width_cm, height_cm)
        
        logger.info(f"ìµœì¢… ì¹˜ìˆ˜: {width_cm:.1f} x {height_cm:.1f}cm (ì‹ ë¢°ë„: {confidence:.2f})")
        
        return {
            'width': round(width_cm, 1),
            'height': round(height_cm, 1),
            'area': round(area_m2, 2),
            'perimeter': round(perimeter_m, 2),
            'confidence': confidence
        }
    
    def calculate_confidence_v2(self, corners, detected_objects, width_cm, height_cm):
        """í–¥ìƒëœ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 1.0
        
        # 1. ê°ì²´ ê°ì§€ ì‹ ë¢°ë„
        if detected_objects:
            obj_confidence = sum(obj['confidence'] for obj in detected_objects) / len(detected_objects)
            confidence *= (0.5 + obj_confidence * 0.5)
        
        # 2. í¬ê¸° í•©ë¦¬ì„± (ì¼ë°˜ì ì¸ ë°© í¬ê¸°: 200-600cm)
        if 200 <= width_cm <= 600 and 200 <= height_cm <= 600:
            confidence *= 1.0
        elif 100 <= width_cm <= 800 and 100 <= height_cm <= 800:
            confidence *= 0.8
        else:
            confidence *= 0.4
        
        # 3. ì¢…íš¡ë¹„ í•©ë¦¬ì„±
        aspect_ratio = max(width_cm, height_cm) / min(width_cm, height_cm)
        if aspect_ratio <= 2.0:
            confidence *= 1.0
        elif aspect_ratio <= 3.0:
            confidence *= 0.8
        else:
            confidence *= 0.5
        
        # 4. ì§ì‚¬ê°í˜• í˜•íƒœ
        rectangularity = self.check_rectangularity(corners)
        confidence *= rectangularity
        
        return max(0.1, min(1.0, confidence))
    
    def check_rectangularity(self, corners):
        """ì§ì‚¬ê°í˜• í˜•íƒœ í™•ì¸"""
        if len(corners) < 4:
            return 0.0
        
        angles = []
        for i in range(4):
            p1 = corners[i]
            p2 = corners[(i + 1) % 4]
            p3 = corners[(i + 2) % 4]
            
            v1 = (p1[0] - p2[0], p1[1] - p2[1])
            v2 = (p3[0] - p2[0], p3[1] - p2[1])
            
            dot_product = v1[0] * v2[0] + v1[1] * v2[1]
            norm1 = math.sqrt(v1[0]**2 + v1[1]**2)
            norm2 = math.sqrt(v2[0]**2 + v2[1]**2)
            
            if norm1 > 0 and norm2 > 0:
                cos_angle = dot_product / (norm1 * norm2)
                cos_angle = max(-1, min(1, cos_angle))
                angle = math.degrees(math.acos(cos_angle))
                angles.append(angle)
        
        rectangularity = 1.0
        for angle in angles:
            diff = abs(angle - 90)
            rectangularity *= max(0.2, 1.0 - diff / 45)
        
        return rectangularity
    
    def smart_visualization(self, image, corners, detected_objects, dimensions):
        """ìŠ¤ë§ˆíŠ¸ ì‹œê°í™”"""
        result = image.copy()
        
        # 1. ê°ì§€ëœ ê°ì²´ í‘œì‹œ
        for obj in detected_objects:
            x, y, w, h = obj['bbox']
            obj_type = obj['type']
            confidence = obj['confidence']
            
            colors = {
                'door': (255, 0, 0),     # ë¹¨ê°•
                'window': (0, 255, 255), # ë…¸ë‘
                'outlet': (255, 0, 255)  # ë§ˆì  íƒ€
            }
            
            color = colors.get(obj_type, (128, 128, 128))
            
            # ê°ì²´ ë°•ìŠ¤
            cv2.rectangle(result, (x, y), (x + w, y + h), color, 3)
            
            # ë¼ë²¨
            label = f"{obj_type} ({confidence:.2f})"
            cv2.putText(result, label, (x, y - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
        
        # 2. ë°© ê²½ê³„ì„  í‘œì‹œ
        if len(corners) >= 4:
            pts = np.array(corners, dtype=np.int32)
            
            # ë°˜íˆ¬ëª… ì˜ì—­
            overlay = result.copy()
            cv2.fillPoly(overlay, [pts], (0, 255, 0))
            result = cv2.addWeighted(result, 0.7, overlay, 0.3, 0)
            
            # ê²½ê³„ì„ 
            cv2.polylines(result, [pts], True, (0, 255, 0), 4)
            
            # ëª¨ì„œë¦¬ì 
            for i, corner in enumerate(corners):
                cv2.circle(result, corner, 12, (255, 0, 0), -1)
                cv2.circle(result, corner, 16, (255, 255, 255), 3)
                cv2.putText(result, str(i+1), 
                           (corner[0] + 20, corner[1] + 20),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # 3. ì •ë³´ íŒ¨ë„ ì¶”ê°€
        if dimensions:
            self.draw_enhanced_info_panel(result, dimensions, detected_objects)
            
            # ì¹˜ìˆ˜ ë¼ë²¨
            if len(corners) >= 4:
                self.draw_dimension_labels_v2(result, corners, dimensions)
        
        # base64 ì¸ì½”ë”©
        _, buffer = cv2.imencode('.png', result)
        result_base64 = base64.b64encode(buffer).decode('utf-8')
        
        return f"data:image/png;base64,{result_base64}"
    
    def draw_enhanced_info_panel(self, image, dimensions, detected_objects):
        """í–¥ìƒëœ ì •ë³´ íŒ¨ë„"""
        panel_height = 160
        panel_width = image.shape[1]
        
        panel = np.zeros((panel_height, panel_width, 3), dtype=np.uint8)
        panel.fill(25)
        
        # ì œëª©
        cv2.putText(panel, "Smart Analysis Results", (20, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
        
        # ì¸¡ì •ê°’
        info_lines = [
            f"Width: {dimensions['width']}cm",
            f"Height: {dimensions['height']}cm",
            f"Area: {dimensions['area']}mÂ²",
            f"Confidence: {dimensions['confidence']:.1%}",
            f"Objects: {len(detected_objects)} detected"
        ]
        
        for i, line in enumerate(info_lines):
            cv2.putText(panel, line, (20, 65 + i * 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        # íŒ¨ë„ì„ ì´ë¯¸ì§€ì— ì¶”ê°€
        result = np.vstack([image, panel])
        image[:] = result[:image.shape[0]]
    
    def draw_dimension_labels_v2(self, image, corners, dimensions):
        """í–¥ìƒëœ ì¹˜ìˆ˜ ë¼ë²¨ ê·¸ë¦¬ê¸°"""
        # ê°€ë¡œ ì¹˜ìˆ˜ (ìƒë‹¨)
        top_center = (
            (corners[0][0] + corners[1][0]) // 2,
            max(30, corners[0][1] - 40)
        )
        
        # ë°°ê²½ ì‚¬ê°í˜•
        text = f"{dimensions['width']}cm"
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]
        cv2.rectangle(image, 
                     (top_center[0] - text_size[0]//2 - 5, top_center[1] - text_size[1] - 5),
                     (top_center[0] + text_size[0]//2 + 5, top_center[1] + 5),
                     (0, 0, 0), -1)
        
        cv2.putText(image, text, 
                   (top_center[0] - text_size[0]//2, top_center[1]),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)
        
        # ì„¸ë¡œ ì¹˜ìˆ˜ (ì¢Œì¸¡)
        left_center = (
            max(80, corners[0][0] - 100),
            (corners[0][1] + corners[3][1]) // 2
        )
        
        text = f"{dimensions['height']}cm"
        text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 2)[0]
        cv2.rectangle(image, 
                     (left_center[0] - text_size[0]//2 - 5, left_center[1] - text_size[1]//2 - 5),
                     (left_center[0] + text_size[0]//2 + 5, left_center[1] + text_size[1]//2 + 5),
                     (0, 0, 0), -1)
        
        cv2.putText(image, text, 
                   (left_center[0] - text_size[0]//2, left_center[1] + text_size[1]//2),
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)

# Flask ë¼ìš°íŠ¸
analyzer = SmartRoomAnalyzer()

@app.route('/')
def index():
    return render_template('room_analyzer.html')

@app.route('/api/analyze', methods=['POST'])
def analyze_room():
    try:
        data = request.get_json()
        
        image_data = data.get('image')
        reference_size = data.get('reference_size', 200)
        options = data.get('options', {})
        
        if not image_data:
            return jsonify({'success': False, 'error': 'ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.'})
        
        logger.info(f"2ë‹¨ê³„ ë¶„ì„ ì‹œì‘ - ê¸°ì¤€ í¬ê¸°: {reference_size}cm")
        result = analyzer.analyze_image(image_data, reference_size, options)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"API ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/health')
def health_check():
    return jsonify({
        'status': 'healthy',
        'version': '2.0.0',
        'features': ['object_detection', 'smart_scaling', 'enhanced_visualization'],
        'timestamp': datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("ğŸš€ 2ë‹¨ê³„ ìŠ¤ë§ˆíŠ¸ ë°© ë¶„ì„ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“ http://localhost:5000 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”")
    print("âœ¨ 2ë‹¨ê³„ ìƒˆë¡œìš´ ê¸°ëŠ¥:")
    print("  - ğŸšª ë¬¸ ìë™ ê°ì§€ (ë†’ì´ ê¸°ì¤€ ìŠ¤ì¼€ì¼ ë³´ì •)")
    print("  - ğŸªŸ ì°½ë¬¸ ìë™ ê°ì§€ (ë°ê¸° + í”„ë ˆì„ ë¶„ì„)")
    print("  - ğŸ”Œ ì½˜ì„¼íŠ¸/ìŠ¤ìœ„ì¹˜ ê°ì§€")
    print("  - ğŸ§  ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ ê³„ì‚°")
    print("  - ğŸ“Š í–¥ìƒëœ ì‹ ë¢°ë„ ì‹œìŠ¤í…œ")
    print("  - ğŸ¨ ê³ ê¸‰ ì‹œê°í™” (ê°ì²´ ë¼ë²¨ë§)")
    app.run(debug=True, host='0.0.0.0', port=5000)