# íŒŒì¼ëª…: app_step4.py
# 4ë‹¨ê³„: AI ê°•í™” ë°© ì „ì²´ ì˜ì—­ ê°ì§€ ì‹œìŠ¤í…œ

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
import cv2
import numpy as np
import base64
import math
from datetime import datetime
import logging
import requests
import os

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

class AIEnhancedRoomAnalyzer:
    def __init__(self):
        # í•œêµ­ ê±´ì¶• í‘œì¤€ ì¹˜ìˆ˜
        self.standard_dimensions = {
            'door': {'width': 80, 'height': 200},
            'window': {'width': 120, 'height': 150},
            'outlet': {'width': 10, 'height': 10}
        }
        
        # AI ëª¨ë¸ ì„¤ì •
        self.setup_ai_models()
        
    def setup_ai_models(self):
        """AI ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            # YOLOv4ë‚˜ ë‹¤ë¥¸ ê°ì²´ ê°ì§€ ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ìˆìŒ
            # ì—¬ê¸°ì„œëŠ” OpenCV DNNì„ ì‚¬ìš©í•œ ì˜ˆì‹œ
            self.net = None
            
            # Google Vision API í‚¤ (ìˆë‹¤ë©´)
            self.vision_api_key = os.getenv('GOOGLE_VISION_API_KEY')
            
            logger.info("AI ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"AI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ë¡œì»¬ CV ì‚¬ìš©: {e}")
            self.net = None
    
    def analyze_image(self, image_data, reference_size=None, options=None):
        """AI ê°•í™” ë¶„ì„"""
        if options is None:
            options = {'detect_windows': True, 'detect_doors': True}
            
        try:
            # ì´ë¯¸ì§€ ë””ì½”ë”©
            image_bytes = base64.b64decode(image_data.split(',')[1])
            image = cv2.imdecode(np.frombuffer(image_bytes, np.uint8), cv2.IMREAD_COLOR)
            
            if image is None:
                raise ValueError("ì´ë¯¸ì§€ë¥¼ ë””ì½”ë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            logger.info(f"AI ê°•í™” ë¶„ì„ ì‹œì‘ - ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
            
            # 1ë‹¨ê³„: AI ê¸°ë°˜ ì‹¤ë‚´ êµ¬ì¡° ë¶„ì„
            structure_analysis = self.ai_structure_analysis(image)
            
            # 2ë‹¨ê³„: ë²½ë©´ vs ê°€êµ¬ êµ¬ë¶„
            wall_mask, furniture_mask = self.separate_walls_and_furniture(image, structure_analysis)
            
            # 3ë‹¨ê³„: AI ê°ì²´ ê°ì§€ (Google Vision API ë˜ëŠ” ë¡œì»¬)
            detected_objects = self.ai_object_detection(image)
            
            # 4ë‹¨ê³„: ë²½ë©´ ê¸°ë°˜ ë°© ê²½ê³„ì„  ê°ì§€
            room_corners = self.detect_room_walls_only(image, wall_mask, furniture_mask)
            
            # 5ë‹¨ê³„: AI ê¸°ë°˜ ìŠ¤ì¼€ì¼ ì¶”ì •
            smart_scale = self.ai_scale_estimation(detected_objects, room_corners, image.shape)
            
            # 6ë‹¨ê³„: ì •í™•í•œ ë°© ì¹˜ìˆ˜ ê³„ì‚°
            room_dimensions = self.calculate_room_dimensions_ai(room_corners, smart_scale)
            
            # 7ë‹¨ê³„: ì‹ ë¢°ë„ ë° ê²€ì¦
            verification_result = self.ai_verification(room_dimensions, detected_objects, structure_analysis)
            
            # 8ë‹¨ê³„: AI ê°•í™” ì‹œê°í™”
            result_image = self.ai_enhanced_visualization(
                image, room_corners, detected_objects, room_dimensions, wall_mask, furniture_mask
            )
            
            return {
                'success': True,
                'dimensions': verification_result['final_dimensions'],
                'detected_objects': detected_objects,
                'structure_analysis': structure_analysis,
                'verification': verification_result,
                'result_image': result_image,
                'analysis_info': {
                    'method': 'ai_enhanced_v4',
                    'ai_features_used': ['structure_analysis', 'wall_furniture_separation', 'object_detection'],
                    'confidence': verification_result['confidence'],
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def ai_structure_analysis(self, image):
        """AI ê¸°ë°˜ ì‹¤ë‚´ êµ¬ì¡° ë¶„ì„"""
        logger.info("AI êµ¬ì¡° ë¶„ì„ ì‹œì‘")
        
        # 1. ìƒ‰ìƒ ê³µê°„ ë¶„ì„ (HSV, LAB)
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
        
        # 2. í…ìŠ¤ì²˜ ë¶„ì„ (LBP - Local Binary Patterns)
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        texture_features = self.analyze_texture_patterns(gray)
        
        # 3. ê¹Šì´ ì¶”ì • (ë‹¨ì•ˆ ì¹´ë©”ë¼ ê¸°ë°˜)
        depth_map = self.estimate_depth(image)
        
        # 4. í‘œë©´ ë¶„ë¥˜ (ë²½ë©´, ë°”ë‹¥, ì²œì¥, ê°€êµ¬)
        surface_classification = self.classify_surfaces(image, hsv, lab, texture_features)
        
        return {
            'texture_features': texture_features,
            'depth_map': depth_map,
            'surface_classification': surface_classification,
            'dominant_colors': self.extract_dominant_colors(image),
            'lighting_analysis': self.analyze_lighting(image)
        }
    
    def analyze_texture_patterns(self, gray):
        """í…ìŠ¤ì²˜ íŒ¨í„´ ë¶„ì„"""
        # LBP (Local Binary Pattern) ê³„ì‚°
        def local_binary_pattern(image, radius=3, n_points=24):
            h, w = image.shape
            lbp = np.zeros((h, w), dtype=np.uint8)
            
            for y in range(radius, h - radius):
                for x in range(radius, w - radius):
                    center = image[y, x]
                    binary_string = ""
                    
                    for i in range(n_points):
                        angle = 2 * np.pi * i / n_points
                        x_offset = int(radius * np.cos(angle))
                        y_offset = int(radius * np.sin(angle))
                        
                        neighbor = image[y + y_offset, x + x_offset]
                        binary_string += "1" if neighbor >= center else "0"
                    
                    lbp[y, x] = int(binary_string, 2) % 256
            
            return lbp
        
        # í…ìŠ¤ì²˜ ë§µ ê³„ì‚°
        lbp = local_binary_pattern(gray)
        
        # í…ìŠ¤ì²˜ ì˜ì—­ ë¶„ë¥˜
        smooth_areas = cv2.inRange(lbp, 0, 50)      # ë§¤ë„ëŸ¬ìš´ ì˜ì—­ (ë²½ë©´)
        textured_areas = cv2.inRange(lbp, 100, 255) # í…ìŠ¤ì²˜ ì˜ì—­ (ê°€êµ¬, íŒ¨í„´)
        
        return {
            'lbp_map': lbp,
            'smooth_areas': smooth_areas,
            'textured_areas': textured_areas
        }
    
    def estimate_depth(self, image):
        """ë‹¨ì•ˆ ì¹´ë©”ë¼ ê¹Šì´ ì¶”ì •"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ê°„ë‹¨í•œ ê¹Šì´ ì¶”ì • (ì‹¤ì œë¡œëŠ” AI ëª¨ë¸ ì‚¬ìš©)
        # ë°ê¸°ì™€ ëŒ€ë¹„ ê¸°ë°˜ ê¹Šì´ ì¶”ì •
        blurred = cv2.GaussianBlur(gray, (21, 21), 0)
        depth_estimate = cv2.absdiff(gray, blurred)
        
        # ì •ê·œí™”
        depth_estimate = cv2.normalize(depth_estimate, None, 0, 255, cv2.NORM_MINMAX)
        
        return depth_estimate
    
    def classify_surfaces(self, image, hsv, lab, texture_features):
        """í‘œë©´ ë¶„ë¥˜ (ë²½, ë°”ë‹¥, ì²œì¥, ê°€êµ¬)"""
        h, w = image.shape[:2]
        classification = np.zeros((h, w), dtype=np.uint8)
        
        # 1. ìœ„ì¹˜ ê¸°ë°˜ ë¶„ë¥˜
        ceiling_mask = np.zeros((h, w), dtype=np.uint8)
        ceiling_mask[0:h//4, :] = 1  # ìƒë‹¨ 25%ëŠ” ì²œì¥ ê°€ëŠ¥ì„±
        
        floor_mask = np.zeros((h, w), dtype=np.uint8)
        floor_mask[3*h//4:h, :] = 1  # í•˜ë‹¨ 25%ëŠ” ë°”ë‹¥ ê°€ëŠ¥ì„±
        
        # 2. ìƒ‰ìƒ ê¸°ë°˜ ë¶„ë¥˜
        # ë²½ë©´ (ë³´í†µ ë°ì€ ìƒ‰ìƒ)
        wall_color_mask = cv2.inRange(hsv[:,:,2], 150, 255)  # ë°ê¸° ê¸°ì¤€
        
        # ë°”ë‹¥ (ë³´í†µ ê°ˆìƒ‰ ê³„ì—´)
        floor_color_mask = cv2.inRange(hsv[:,:,0], 10, 25)   # ê°ˆìƒ‰ ìƒ‰ì¡°
        
        # 3. í…ìŠ¤ì²˜ ê¸°ë°˜ ë¶„ë¥˜
        smooth_mask = texture_features['smooth_areas']
        textured_mask = texture_features['textured_areas']
        
        # 4. ì¢…í•© ë¶„ë¥˜
        # ë²½ë©´: ë°ê³  ë§¤ë„ëŸ¬ìš´ ì˜ì—­
        wall_mask = cv2.bitwise_and(wall_color_mask, smooth_mask)
        wall_mask = cv2.bitwise_and(wall_mask, cv2.bitwise_not(floor_mask))
        
        # ê°€êµ¬: í…ìŠ¤ì²˜ê°€ ìˆëŠ” ì˜ì—­
        furniture_mask = textured_mask.copy()
        
        # ë°”ë‹¥: í•˜ë‹¨ + íŠ¹ì • ìƒ‰ìƒ
        floor_final_mask = cv2.bitwise_and(floor_mask, 
                                          cv2.bitwise_or(floor_color_mask, smooth_mask))
        
        # ì²œì¥: ìƒë‹¨ + ë°ì€ ìƒ‰ìƒ
        ceiling_final_mask = cv2.bitwise_and(ceiling_mask, wall_color_mask)
        
        return {
            'wall_mask': wall_mask,
            'furniture_mask': furniture_mask,
            'floor_mask': floor_final_mask,
            'ceiling_mask': ceiling_final_mask
        }
    
    def extract_dominant_colors(self, image):
        """ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ"""
        # K-means í´ëŸ¬ìŠ¤í„°ë§ìœ¼ë¡œ ì£¼ìš” ìƒ‰ìƒ ì¶”ì¶œ
        data = image.reshape((-1, 3))
        data = np.float32(data)
        
        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 20, 1.0)
        k = 5
        _, labels, centers = cv2.kmeans(data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)
        
        return centers.astype(np.uint8).tolist()
    
    def analyze_lighting(self, image):
        """ì¡°ëª… ë¶„ì„"""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # ë°ê¸° ë¶„í¬
        brightness_mean = np.mean(gray)
        brightness_std = np.std(gray)
        
        # ë°ì€ ì˜ì—­ (ì°½ë¬¸, ì¡°ëª…)
        bright_areas = cv2.inRange(gray, 200, 255)
        bright_ratio = np.sum(bright_areas > 0) / (image.shape[0] * image.shape[1])
        
        return {
            'brightness_mean': float(brightness_mean),
            'brightness_std': float(brightness_std),
            'bright_areas_ratio': float(bright_ratio)
        }
    
    def separate_walls_and_furniture(self, image, structure_analysis):
        """ë²½ë©´ê³¼ ê°€êµ¬ ë¶„ë¦¬"""
        surface_class = structure_analysis['surface_classification']
        
        # ë²½ë©´ ë§ˆìŠ¤í¬ (ë²½ + ì²œì¥)
        wall_mask = cv2.bitwise_or(surface_class['wall_mask'], surface_class['ceiling_mask'])
        
        # ê°€êµ¬ ë§ˆìŠ¤í¬
        furniture_mask = surface_class['furniture_mask']
        
        # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ì •ì œ
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))
        wall_mask = cv2.morphologyEx(wall_mask, cv2.MORPH_CLOSE, kernel)
        furniture_mask = cv2.morphologyEx(furniture_mask, cv2.MORPH_OPEN, kernel)
        
        logger.info(f"ë²½ë©´/ê°€êµ¬ ë¶„ë¦¬ ì™„ë£Œ - ë²½ë©´: {np.sum(wall_mask>0)} pixels, ê°€êµ¬: {np.sum(furniture_mask>0)} pixels")
        
        return wall_mask, furniture_mask
    
    def ai_object_detection(self, image):
        """AI ê¸°ë°˜ ê°ì²´ ê°ì§€"""
        detected_objects = []
        
        try:
            # Google Vision API ì‚¬ìš© (API í‚¤ê°€ ìˆëŠ” ê²½ìš°)
            if self.vision_api_key:
                vision_objects = self.google_vision_detection(image)
                detected_objects.extend(vision_objects)
                logger.info(f"Google Vision API: {len(vision_objects)}ê°œ ê°ì²´ ê°ì§€")
            
            # ë¡œì»¬ AI ëª¨ë¸ ë°±ì—…
            local_objects = self.enhanced_local_detection(image)
            detected_objects.extend(local_objects)
            logger.info(f"ë¡œì»¬ AI: {len(local_objects)}ê°œ ê°ì²´ ê°ì§€")
            
        except Exception as e:
            logger.warning(f"AI ê°ì²´ ê°ì§€ ì˜¤ë¥˜: {e}")
            # ë°±ì—… ë°©ë²•
            detected_objects = self.fallback_object_detection(image)
        
        return detected_objects
    
    def google_vision_detection(self, image):
        """Google Vision API ê°ì²´ ê°ì§€"""
        if not self.vision_api_key:
            return []
        
        try:
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            _, buffer = cv2.imencode('.jpg', image)
            image_base64 = base64.b64encode(buffer).decode()
            
            # API í˜¸ì¶œ
            url = f'https://vision.googleapis.com/v1/images:annotate?key={self.vision_api_key}'
            
            payload = {
                'requests': [{
                    'image': {'content': image_base64},
                    'features': [
                        {'type': 'OBJECT_LOCALIZATION', 'maxResults': 50}
                    ]
                }]
            }
            
            response = requests.post(url, json=payload, timeout=10)
            result = response.json()
            
            objects = []
            if 'responses' in result and result['responses']:
                if 'localizedObjectAnnotations' in result['responses'][0]:
                    for obj in result['responses'][0]['localizedObjectAnnotations']:
                        # ì‹¤ë‚´ ê´€ë ¨ ê°ì²´ë§Œ í•„í„°ë§
                        if obj['name'].lower() in ['door', 'window', 'furniture', 'cabinet', 'refrigerator']:
                            bbox = obj['boundingPoly']['normalizedVertices']
                            h, w = image.shape[:2]
                            
                            x1 = int(bbox[0].get('x', 0) * w)
                            y1 = int(bbox[0].get('y', 0) * h)
                            x2 = int(bbox[2].get('x', 1) * w)
                            y2 = int(bbox[2].get('y', 1) * h)
                            
                            objects.append({
                                'type': obj['name'].lower(),
                                'bbox': (x1, y1, x2-x1, y2-y1),
                                'confidence': obj['score'],
                                'method': 'google_vision_ai'
                            })
            
            return objects
            
        except Exception as e:
            logger.error(f"Google Vision API ì˜¤ë¥˜: {e}")
            return []
    
    def enhanced_local_detection(self, image):
        """ê°•í™”ëœ ë¡œì»¬ ê°ì²´ ê°ì§€"""
        objects = []
        
        # 1. ë¬¸ ê°ì§€ (AI ê°•í™”)
        doors = self.ai_enhanced_door_detection(image)
        objects.extend(doors)
        
        # 2. ì°½ë¬¸ ê°ì§€ (AI ê°•í™”)
        windows = self.ai_enhanced_window_detection(image)
        objects.extend(windows)
        
        # 3. ê°€êµ¬ ê°ì§€ (ìƒˆë¡œ ì¶”ê°€)
        furniture = self.ai_furniture_detection(image)
        objects.extend(furniture)
        
        return objects
    
    def ai_enhanced_door_detection(self, image):
        """AI ê°•í™” ë¬¸ ê°ì§€"""
        doors = []
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 1. ì—£ì§€ ê¸°ë°˜ ê²€ì¶œ
        edges = cv2.Canny(gray, 50, 150)
        
        # 2. ìˆ˜ì§ì„  ê°•ì¡° í•„í„°
        kernel_vertical = np.array([[-1, 2, -1], [-1, 2, -1], [-1, 2, -1]], dtype=np.float32)
        vertical_edges = cv2.filter2D(edges, -1, kernel_vertical)
        
        # 3. ì»¨íˆ¬ì–´ ê²€ì¶œ
        contours, _ = cv2.findContours(vertical_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 3000 < area < 100000:
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = h / w
                
                if 2.0 < aspect_ratio < 4.0:  # ë¬¸ì˜ ì¢…íš¡ë¹„
                    # AI íŠ¹ì§• ê²€ì¦
                    confidence = self.verify_door_with_ai(image[y:y+h, x:x+w])
                    
                    if confidence > 0.5:
                        doors.append({
                            'type': 'door',
                            'bbox': (x, y, w, h),
                            'confidence': confidence,
                            'method': 'ai_enhanced_local',
                            'estimated_real_size': {'width': 80, 'height': 200}
                        })
        
        return doors
    
    def verify_door_with_ai(self, door_roi):
        """AI ê¸°ë°˜ ë¬¸ ê²€ì¦"""
        if door_roi.size == 0:
            return 0.0
        
        confidence = 0.0
        
        # 1. ìƒ‰ìƒ ë¶„ì„ (ë¬¸ íŠ¹ì§•ì  ìƒ‰ìƒ)
        hsv = cv2.cvtColor(door_roi, cv2.COLOR_BGR2HSV)
        
        # ê°ˆìƒ‰ ê³„ì—´ (ë‚˜ë¬´ë¬¸)
        brown_mask = cv2.inRange(hsv, np.array([5, 30, 20]), np.array([25, 255, 200]))
        brown_ratio = np.sum(brown_mask > 0) / door_roi.size
        
        # í°ìƒ‰/íšŒìƒ‰ ê³„ì—´ (ë„ì¥ë¬¸)
        white_mask = cv2.inRange(hsv, np.array([0, 0, 150]), np.array([180, 30, 255]))
        white_ratio = np.sum(white_mask > 0) / door_roi.size
        
        color_confidence = min(1.0, (brown_ratio + white_ratio) * 2)
        confidence += color_confidence * 0.4
        
        # 2. ì—£ì§€ íŒ¨í„´ ë¶„ì„
        gray_roi = cv2.cvtColor(door_roi, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray_roi, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        if 0.05 < edge_density < 0.25:
            confidence += 0.3
        
        # 3. ìˆ˜ì§ì„± ê²€ì¦
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=20, minLineLength=30, maxLineGap=10)
        if lines is not None:
            vertical_score = 0
            for line in lines:
                x1, y1, x2, y2 = line[0]
                angle = abs(math.degrees(math.atan2(y2-y1, x2-x1)))
                if 80 < angle < 100:  # ìˆ˜ì§ì„ 
                    vertical_score += 1
            
            if vertical_score >= 2:
                confidence += 0.3
        
        return min(1.0, confidence)
    
    def ai_enhanced_window_detection(self, image):
        """AI ê°•í™” ì°½ë¬¸ ê°ì§€"""
        windows = []
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 1. ë°ê¸° ê¸°ë°˜ ê²€ì¶œ (ì°½ë¬¸ = ìì—°ê´‘)
        _, bright_mask = cv2.threshold(gray, 140, 255, cv2.THRESH_BINARY)
        
        # 2. ëª¨í´ë¡œì§€ ì—°ì‚°
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))
        bright_mask = cv2.morphologyEx(bright_mask, cv2.MORPH_CLOSE, kernel)
        bright_mask = cv2.morphologyEx(bright_mask, cv2.MORPH_OPEN, kernel)
        
        contours, _ = cv2.findContours(bright_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 8000 < area < 200000:  # ì°½ë¬¸ í¬ê¸°
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = w / h
                
                if 0.8 < aspect_ratio < 3.0:  # ì°½ë¬¸ ì¢…íš¡ë¹„
                    confidence = self.verify_window_with_ai(image[y:y+h, x:x+w], gray[y:y+h, x:x+w])
                    
                    if confidence > 0.6:
                        windows.append({
                            'type': 'window',
                            'bbox': (x, y, w, h),
                            'confidence': confidence,
                            'method': 'ai_enhanced_local',
                            'estimated_real_size': {'width': 120, 'height': 150}
                        })
        
        return windows
    
    def verify_window_with_ai(self, window_roi, gray_roi):
        """AI ê¸°ë°˜ ì°½ë¬¸ ê²€ì¦"""
        if window_roi.size == 0:
            return 0.0
        
        confidence = 0.0
        
        # 1. ë°ê¸° ë¶„ì„
        mean_brightness = np.mean(gray_roi)
        if mean_brightness > 130:
            confidence += 0.4
        
        # 2. í”„ë ˆì„ ê²€ì¶œ
        edges = cv2.Canny(gray_roi, 30, 100)
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=15, minLineLength=20, maxLineGap=5)
        
        if lines is not None:
            horizontal_lines = 0
            vertical_lines = 0
            
            for line in lines:
                x1, y1, x2, y2 = line[0]
                angle = abs(math.degrees(math.atan2(y2-y1, x2-x1)))
                
                if angle < 20 or angle > 160:
                    horizontal_lines += 1
                elif 70 < angle < 110:
                    vertical_lines += 1
            
            if horizontal_lines >= 2 and vertical_lines >= 2:
                confidence += 0.4
        
        # 3. ìœ ë¦¬ ë°˜ì‚¬ íŠ¹ì„±
        brightness_std = np.std(gray_roi)
        if 20 < brightness_std < 70:
            confidence += 0.2
        
        return min(1.0, confidence)
    
    def ai_furniture_detection(self, image):
        """AI ê°€êµ¬ ê°ì§€"""
        furniture = []
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 1. í…ìŠ¤ì²˜ ê¸°ë°˜ ê°€êµ¬ ê°ì§€
        # ê°€êµ¬ëŠ” ë³´í†µ ê· ì¼í•˜ì§€ ì•Šì€ í…ìŠ¤ì²˜ë¥¼ ê°€ì§
        
        # 2. ìƒ‰ìƒ ê¸°ë°˜ ê°€êµ¬ ê°ì§€
        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
        
        # ê°€êµ¬ ìƒ‰ìƒ ë²”ìœ„ (ê°ˆìƒ‰, ê²€ì •, í°ìƒ‰ ë“±)
        furniture_colors = [
            (np.array([5, 50, 50]), np.array([25, 255, 200])),    # ê°ˆìƒ‰
            (np.array([0, 0, 0]), np.array([180, 255, 50]),       # ê²€ì •
            (np.array([0, 0, 200]), np.array([180, 30, 255]))     # í°ìƒ‰
        ]
        
        combined_mask = np.zeros(gray.shape, dtype=np.uint8)
        for lower, upper in furniture_colors:
            mask = cv2.inRange(hsv, lower, upper)
            combined_mask = cv2.bitwise_or(combined_mask, mask)
        
        # 3. ì»¨íˆ¬ì–´ ê²€ì¶œ
        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for contour in contours:
            area = cv2.contourArea(contour)
            if 5000 < area < 150000:  # ê°€êµ¬ í¬ê¸° ë²”ìœ„
                x, y, w, h = cv2.boundingRect(contour)
                
                furniture.append({
                    'type': 'furniture',
                    'bbox': (x, y, w, h),
                    'confidence': 0.7,
                    'method': 'ai_color_texture'
                })
        
        return furniture
    
    def fallback_object_detection(self, image):
        """ë°±ì—… ê°ì²´ ê°ì§€"""
        # ê¸°ì¡´ì˜ ê°„ë‹¨í•œ ê°ì²´ ê°ì§€ ë°©ë²•
        return []
    
    def detect_room_walls_only(self, image, wall_mask, furniture_mask):
        """ë²½ë©´ë§Œì„ ì´ìš©í•œ ë°© ê²½ê³„ì„  ê°ì§€"""
        logger.info("ë²½ë©´ ê¸°ë°˜ ë°© ê²½ê³„ì„  ê°ì§€ ì‹œì‘")
        
        # 1. ê°€êµ¬ ì˜ì—­ì„ ì œì™¸í•œ ë²½ë©´ë§Œ ì‚¬ìš©
        clean_wall_mask = cv2.bitwise_and(wall_mask, cv2.bitwise_not(furniture_mask))
        
        # 2. ë²½ë©´ ì˜ì—­ì—ì„œ ì—£ì§€ ê²€ì¶œ
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 30, 100)
        
        # ë²½ë©´ ë§ˆìŠ¤í¬ ì ìš©
        wall_edges = cv2.bitwise_and(edges, clean_wall_mask)
        
        # 3. ì´ë¯¸ì§€ ê²½ê³„ë¶€ ê°•í™” (ë²½ë©´ì€ ë³´í†µ ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ì— ìˆìŒ)
        h, w = image.shape[:2]
        border_mask = np.zeros((h, w), dtype=np.uint8)
        border_width = min(w, h) // 10
        
        # ê²½ê³„ë¶€ ë§ˆìŠ¤í¬ ìƒì„±
        border_mask[0:border_width, :] = 255          # ìƒë‹¨
        border_mask[h-border_width:h, :] = 255        # í•˜ë‹¨
        border_mask[:, 0:border_width] = 255          # ì¢Œì¸¡
        border_mask[:, w-border_width:w] = 255        # ìš°ì¸¡
        
        # ê²½ê³„ë¶€ì™€ ë²½ë©´ ì—£ì§€ ê²°í•©
        enhanced_edges = cv2.bitwise_or(wall_edges, cv2.bitwise_and(edges, border_mask))
        
        # 4. ì§ì„  ê²€ì¶œ (íŒŒë¼ë¯¸í„° ì¡°ì •)
        lines = cv2.HoughLinesP(
            enhanced_edges,
            rho=1,
            theta=np.pi/180,
            threshold=40,
            minLineLength=min(w, h) // 8,  # ì´ë¯¸ì§€ í¬ê¸° ëŒ€ë¹„ ìµœì†Œ ê¸¸ì´
            maxLineGap=50
        )
        
        if lines is None:
            logger.warning("ì§ì„ ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ê¸°ë³¸ ëª¨ì„œë¦¬ ì‚¬ìš©")
            return self.generate_default_room_corners(image.shape)
        
        # 5. ë°© ì „ì²´ ê²½ê³„ì„  ì¶”ì¶œ
        room_corners = self.extract_full_room_boundaries(lines, image.shape, wall_mask)
        
        logger.info(f"ë²½ë©´ ê¸°ë°˜ ëª¨ì„œë¦¬ ê°ì§€ ì™„ë£Œ: {len(room_corners)}ê°œ")
        
        return room_corners
    
    def extract_full_room_boundaries(self, lines, image_shape, wall_mask):
        """ë°© ì „ì²´ ê²½ê³„ì„  ì¶”ì¶œ"""
        h, w = image_shape[:2]
        
        # 1. ì§ì„ ì„ ìˆ˜í‰ì„ ê³¼ ìˆ˜ì§ì„ ìœ¼ë¡œ ë¶„ë¥˜
        horizontal_lines = []
        vertical_lines = []
        
        for line in lines:
            x1, y1, x2, y2 = line[0]
            length = math.sqrt((x2 - x1)**2 + (y2 - y1)**2)
            angle = math.degrees(math.atan2(y2 - y1, x2 - x1))
            
            # ì¶©ë¶„íˆ ê¸´ ì§ì„ ë§Œ ê³ ë ¤
            if length > min(w, h) / 10:
                if abs(angle) < 25 or abs(angle) > 155:  # ìˆ˜í‰ì„  (ê°ë„ ë²”ìœ„ í™•ëŒ€)
                    horizontal_lines.append((line[0], length))
                elif 65 < abs(angle) < 115:  # ìˆ˜ì§ì„  (ê°ë„ ë²”ìœ„ í™•ëŒ€)
                    vertical_lines.append((line[0], length))
        
        logger.info(f"ìˆ˜í‰ì„ : {len(horizontal_lines)}ê°œ, ìˆ˜ì§ì„ : {len(vertical_lines)}ê°œ")
        
        # 2. ë°©ì˜ ì™¸ê³½ ê²½ê³„ì„  ì„ íƒ
        room_boundaries = self.select_room_outer_boundaries(horizontal_lines, vertical_lines, w, h)
        
        # 3. êµì  ê³„ì‚°
        corners = []
        if all(boundary is not None for boundary in room_boundaries.values()):
            corners = self.calculate_room_intersections(room_boundaries)
        
        # 4. ëª¨ì„œë¦¬ì ì´ ë¶€ì¡±í•˜ë©´ ì´ë¯¸ì§€ ê²½ê³„ ì‚¬ìš©
        if len(corners) < 4:
            corners = self.generate_room_corners_with_boundaries(image_shape, lines)
        
        return corners[:4]
    
    def select_room_outer_boundaries(self, h_lines, v_lines, width, height):
        """ë°©ì˜ ì™¸ê³½ ê²½ê³„ì„  ì„ íƒ"""
        boundaries = {'top': None, 'bottom': None, 'left': None, 'right': None}
        
        # ìˆ˜í‰ì„ ì—ì„œ ìµœìƒë‹¨ê³¼ ìµœí•˜ë‹¨ ì„ íƒ
        if h_lines:
            # ê¸¸ì´ ê°€ì¤‘ì¹˜ ê³ ë ¤í•œ ì„ íƒ
            h_lines_sorted = sorted(h_lines, key=lambda x: x[1], reverse=True)  # ê¸¸ì´ìˆœ ì •ë ¬
            top_candidates = [line for line, length in h_lines_sorted if (line[1] + line[3]) / 2 < height / 2]
            bottom_candidates = [line for line, length in h_lines_sorted if (line[1] + line[3]) / 2 > height / 2]
            
            if top_candidates:
                boundaries['top'] = min(top_candidates, key=lambda l: (l[1] + l[3]) / 2)
            if bottom_candidates:
                boundaries['bottom'] = max(bottom_candidates, key=lambda l: (l[1] + l[3]) / 2)
        
        # ìˆ˜ì§ì„ ì—ì„œ ìµœì¢Œì¸¡ê³¼ ìµœìš°ì¸¡ ì„ íƒ
        if v_lines:
            v_lines_sorted = sorted(v_lines, key=lambda x: x[1], reverse=True)  # ê¸¸ì´ìˆœ ì •ë ¬
            left_candidates = [line for line, length in v_lines_sorted if (line[0] + line[2]) / 2 < width / 2]
            right_candidates = [line for line, length in v_lines_sorted if (line[0] + line[2]) / 2 > width / 2]
            
            if left_candidates:
                boundaries['left'] = min(left_candidates, key=lambda l: (l[0] + l[2]) / 2)
            if right_candidates:
                boundaries['right'] = max(right_candidates, key=lambda l: (l[0] + l[2]) / 2)
        
        return boundaries
    
    def calculate_room_intersections(self, boundaries):
        """ë°© ê²½ê³„ì„ ë“¤ì˜ êµì  ê³„ì‚°"""
        corners = []
        
        intersections = [
            ('top', 'left'),
            ('top', 'right'),
            ('bottom', 'right'),
            ('bottom', 'left')
        ]
        
        for h_key, v_key in intersections:
            if boundaries[h_key] is not None and boundaries[v_key] is not None:
                intersection = self.line_intersection(boundaries[h_key], boundaries[v_key])
                if intersection:
                    corners.append(intersection)
        
        return corners
    
    def generate_room_corners_with_boundaries(self, image_shape, lines):
        """ê²½ê³„ì„ ì„ ì´ìš©í•œ ë°© ëª¨ì„œë¦¬ ìƒì„±"""
        h, w = image_shape[:2]
        
        # ì´ë¯¸ì§€ ê²½ê³„ì—ì„œ ì¼ì • ê±°ë¦¬ ì•ˆìª½ì— ëª¨ì„œë¦¬ ìƒì„±
        margin_x = w * 0.05  # 5% ì—¬ë°±
        margin_y = h * 0.05
        
        # ê°ì§€ëœ ì§ì„ ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì—¬ë°± ì¡°ì •
        if lines is not None and len(lines) > 0:
            # ê°€ì¥ ë°”ê¹¥ìª½ ì§ì„ ë“¤ì˜ ìœ„ì¹˜ë¥¼ ì°¸ê³ 
            all_x = []
            all_y = []
            
            for line in lines:
                x1, y1, x2, y2 = line[0]
                all_x.extend([x1, x2])
                all_y.extend([y1, y2])
            
            if all_x and all_y:
                min_x, max_x = min(all_x), max(all_x)
                min_y, max_y = min(all_y), max(all_y)
                
                # ì‹¤ì œ ê°ì§€ëœ ë²”ìœ„ ì‚¬ìš©
                margin_x = max(margin_x, min_x)
                margin_y = max(margin_y, min_y)
                max_x = min(max_x, w - margin_x)
                max_y = min(max_y, h - margin_y)
                
                return [
                    (int(margin_x), int(margin_y)),
                    (int(max_x), int(margin_y)),
                    (int(max_x), int(max_y)),
                    (int(margin_x), int(max_y))
                ]
        
        # ê¸°ë³¸ ëª¨ì„œë¦¬
        return [
            (int(margin_x), int(margin_y)),
            (int(w - margin_x), int(margin_y)),
            (int(w - margin_x), int(h - margin_y)),
            (int(margin_x), int(h - margin_y))
        ]
    
    def generate_default_room_corners(self, image_shape):
        """ê¸°ë³¸ ë°© ëª¨ì„œë¦¬ ìƒì„±"""
        h, w = image_shape[:2]
        margin = min(w, h) * 0.08  # 8% ì—¬ë°±
        
        return [
            (int(margin), int(margin)),
            (int(w - margin), int(margin)),
            (int(w - margin), int(h - margin)),
            (int(margin), int(h - margin))
        ]
    
    def line_intersection(self, line1, line2):
        """ë‘ ì§ì„ ì˜ êµì  ê³„ì‚°"""
        x1, y1, x2, y2 = line1
        x3, y3, x4, y4 = line2
        
        denom = (x1 - x2) * (y3 - y4) - (y1 - y2) * (x3 - x4)
        if abs(denom) < 1e-10:
            return None
        
        t = ((x1 - x3) * (y3 - y4) - (y1 - y3) * (x3 - x4)) / denom
        
        x = x1 + t * (x2 - x1)
        y = y1 + t * (y2 - y1)
        
        return (int(x), int(y))
    
    def ai_scale_estimation(self, detected_objects, room_corners, image_shape):
        """AI ê¸°ë°˜ ìŠ¤ì¼€ì¼ ì¶”ì •"""
        scale_candidates = []
        
        # 1. ê°ì²´ ê¸°ë°˜ ìŠ¤ì¼€ì¼ ê³„ì‚°
        for obj in detected_objects:
            if 'estimated_real_size' in obj:
                obj_type = obj['type']
                bbox = obj['bbox']
                real_size = obj['estimated_real_size']
                confidence = obj['confidence']
                
                if obj_type == 'door':
                    # ë¬¸ ë†’ì´ ê¸°ì¤€ (ê°€ì¥ ì‹ ë¢°í•  ë§Œí•¨)
                    scale = real_size['height'] / bbox[3]  # height
                    scale_candidates.append({
                        'scale': scale,
                        'confidence': confidence * 0.9,
                        'source': f'door_height_{real_size["height"]}cm'
                    })
                
                elif obj_type == 'window':
                    # ì°½ë¬¸ ë„ˆë¹„ ê¸°ì¤€
                    scale = real_size['width'] / bbox[2]  # width
                    scale_candidates.append({
                        'scale': scale,
                        'confidence': confidence * 0.8,
                        'source': f'window_width_{real_size["width"]}cm'
                    })
        
        # 2. ë°© ë¹„ìœ¨ ê¸°ë°˜ ì¶”ì • (í•œêµ­ ì£¼ê±° í‘œì¤€)
        if room_corners and len(room_corners) >= 4:
            # ë°©ì˜ í”½ì…€ í¬ê¸°
            tl, tr, br, bl = room_corners[:4]
            width_pixels = math.sqrt((tr[0] - tl[0])**2 + (tr[1] - tl[1])**2)
            height_pixels = math.sqrt((bl[0] - tl[0])**2 + (bl[1] - tl[1])**2)
            
            # ì¼ë°˜ì ì¸ ë°© í¬ê¸° ì‹œë‚˜ë¦¬ì˜¤ë“¤
            typical_room_sizes = [
                (300, 400),  # 3m x 4m
                (350, 450),  # 3.5m x 4.5m
                (400, 500),  # 4m x 5m
                (250, 350),  # 2.5m x 3.5m
            ]
            
            for real_width, real_height in typical_room_sizes:
                # ë‘ ê°€ì§€ ë°©í–¥ ê³ ë ¤
                scale1 = real_width / width_pixels
                scale2 = real_height / height_pixels
                avg_scale = (scale1 + scale2) / 2
                
                scale_candidates.append({
                    'scale': avg_scale,
                    'confidence': 0.4,
                    'source': f'typical_room_{real_width}x{real_height}cm'
                })
        
        # 3. ìµœì  ìŠ¤ì¼€ì¼ ì„ íƒ
        if scale_candidates:
            # ì‹ ë¢°ë„ ê°€ì¤‘í‰ê· 
            total_weight = sum(sc['confidence'] for sc in scale_candidates)
            if total_weight > 0:
                best_scale = sum(sc['scale'] * sc['confidence'] for sc in scale_candidates) / total_weight
            else:
                best_scale = scale_candidates[0]['scale']
        else:
            # ê¸°ë³¸ê°’
            h, w = image_shape[:2]
            best_scale = 350 / min(w, h)  # 3.5m ê°€ì •
        
        logger.info(f"AI ìŠ¤ì¼€ì¼ ì¶”ì •: {best_scale:.4f} cm/pixel ({len(scale_candidates)}ê°œ í›„ë³´)")
        
        return best_scale
    
    def calculate_room_dimensions_ai(self, corners, scale):
        """AI ê¸°ë°˜ ë°© ì¹˜ìˆ˜ ê³„ì‚°"""
        if len(corners) < 4:
            return {'width': 0, 'height': 0, 'area': 0, 'perimeter': 0, 'confidence': 0.0}
        
        # ëª¨ì„œë¦¬ì ë“¤ì„ ì •ë ¬
        tl, tr, br, bl = corners[:4]
        
        # í”½ì…€ ê±°ë¦¬ ê³„ì‚°
        width_pixels = math.sqrt((tr[0] - tl[0])**2 + (tr[1] - tl[1])**2)
        height_pixels = math.sqrt((bl[0] - tl[0])**2 + (bl[1] - tl[1])**2)
        
        # ì‹¤ì œ í¬ê¸° ê³„ì‚°
        width_cm = width_pixels * scale
        height_cm = height_pixels * scale
        area_m2 = (width_cm * height_cm) / 10000
        perimeter_m = (width_cm + height_cm) * 2 / 100
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self.calculate_ai_confidence(corners, width_cm, height_cm)
        
        return {
            'width': round(width_cm, 1),
            'height': round(height_cm, 1),
            'area': round(area_m2, 2),
            'perimeter': round(perimeter_m, 2),
            'confidence': confidence
        }
    
    def calculate_ai_confidence(self, corners, width_cm, height_cm):
        """AI ê¸°ë°˜ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = 1.0
        
        # 1. í¬ê¸° í•©ë¦¬ì„± (í•œêµ­ ì£¼ê±° ê¸°ì¤€)
        if 250 <= width_cm <= 600 and 200 <= height_cm <= 500:
            confidence *= 1.0
        elif 200 <= width_cm <= 800 and 150 <= height_cm <= 600:
            confidence *= 0.8
        else:
            confidence *= 0.5
        
        # 2. ì¢…íš¡ë¹„ í•©ë¦¬ì„±
        aspect_ratio = max(width_cm, height_cm) / min(width_cm, height_cm)
        if aspect_ratio <= 2.0:
            confidence *= 1.0
        elif aspect_ratio <= 2.5:
            confidence *= 0.8
        else:
            confidence *= 0.6
        
        # 3. ì§ì‚¬ê°í˜• í˜•íƒœ
        rectangularity = self.check_rectangularity_ai(corners)
        confidence *= rectangularity
        
        return max(0.2, min(1.0, confidence))
    
    def check_rectangularity_ai(self, corners):
        """AI ì§ì‚¬ê°í˜• í˜•íƒœ ê²€ì¦"""
        if len(corners) < 4:
            return 0.0
        
        # ê° ë‚´ê°ì´ 90ë„ì— ê°€ê¹Œìš´ì§€ í™•ì¸
        angles = []
        for i in range(4):
            p1 = corners[i]
            p2 = corners[(i + 1) % 4]
            p3 = corners[(i + 2) % 4]
            
            v1 = (p1[0] - p2[0], p1[1] - p2[1])
            v2 = (p3[0] - p2[0], p3[1] - p2[1])
            
            dot_product = v1[0] * v2[0] + v1[1] * v2[1]
            norm1 = math.sqrt(v1[0]**2 + v1[1]**2)
            norm2 = math.sqrt(v2[0]**2 + v2[1]**2)
            
            if norm1 > 0 and norm2 > 0:
                cos_angle = max(-1, min(1, dot_product / (norm1 * norm2)))
                angle = math.degrees(math.acos(cos_angle))
                angles.append(angle)
        
        # 90ë„ì™€ì˜ ì°¨ì´ë¡œ ì ìˆ˜ ê³„ì‚°
        rectangularity = 1.0
        for angle in angles:
            diff = abs(angle - 90)
            rectangularity *= max(0.3, 1.0 - diff / 60)  # 60ë„ ì°¨ì´ê¹Œì§€ í—ˆìš©
        
        return rectangularity
    
    def ai_verification(self, dimensions, detected_objects, structure_analysis):
        """AI ê¸°ë°˜ ê²°ê³¼ ê²€ì¦"""
        verification = {
            'final_dimensions': dimensions,
            'confidence': dimensions['confidence'],
            'warnings': [],
            'recommendations': []
        }
        
        # 1. í¬ê¸° ê²€ì¦
        area = dimensions['area']
        if area < 5:
            verification['warnings'].append("ë°© í¬ê¸°ê°€ ë§¤ìš° ì‘ìŠµë‹ˆë‹¤ (5mÂ² ë¯¸ë§Œ)")
            verification['recommendations'].append("ì°½ë¬¸ì´ë‚˜ ë¬¸ì„ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì¸¡ì •í•´ë³´ì„¸ìš”")
        elif area > 50:
            verification['warnings'].append("ë°© í¬ê¸°ê°€ ë§¤ìš° í½ë‹ˆë‹¤ (50mÂ² ì´ˆê³¼)")
            verification['recommendations'].append("ì¸¡ì • ì˜ì—­ì´ ë°© ì „ì²´ê°€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
        
        # 2. ê°ì²´ ì¼ê´€ì„± ê²€ì¦
        if detected_objects:
            object_scales = []
            for obj in detected_objects:
                if 'estimated_real_size' in obj and obj['type'] in ['door', 'window']:
                    bbox = obj['bbox']
                    real_size = obj['estimated_real_size']
                    
                    if obj['type'] == 'door':
                        scale = real_size['height'] / bbox[3]
                    else:
                        scale = real_size['width'] / bbox[2]
                    
                    object_scales.append(scale)
            
            if len(object_scales) > 1:
                scale_std = np.std(object_scales)
                if scale_std > np.mean(object_scales) * 0.3:
                    verification['warnings'].append("ê°ì²´ ê¸°ë°˜ ìŠ¤ì¼€ì¼ì´ ì¼ê´€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        # 3. êµ¬ì¡° ë¶„ì„ ì¼ê´€ì„±
        brightness = structure_analysis['lighting_analysis']['brightness_mean']
        if brightness < 100:
            verification['warnings'].append("ì´ë¯¸ì§€ê°€ ì–´ë‘ì›Œ ë¶„ì„ ì •í™•ë„ê°€ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            verification['recommendations'].append("ë” ë°ì€ í™˜ê²½ì—ì„œ ì´¬ì˜í•œ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”")
        
        return verification
    
    def ai_enhanced_visualization(self, image, corners, detected_objects, dimensions, wall_mask, furniture_mask):
        """AI ê°•í™” ì‹œê°í™”"""
        result = image.copy()
        
        # 1. êµ¬ì¡° ë¶„ì„ ê²°ê³¼ í‘œì‹œ (ë²½ë©´/ê°€êµ¬ êµ¬ë¶„)
        # ë²½ë©´ ì˜ì—­ì„ ì—°í•œ íŒŒë€ìƒ‰ìœ¼ë¡œ í‘œì‹œ
        wall_overlay = result.copy()
        wall_overlay[wall_mask > 0] = [255, 200, 200]  # ì—°í•œ íŒŒë€ìƒ‰
        result = cv2.addWeighted(result, 0.85, wall_overlay, 0.15, 0)
        
        # ê°€êµ¬ ì˜ì—­ì„ ì—°í•œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ í‘œì‹œ
        furniture_overlay = result.copy()
        furniture_overlay[furniture_mask > 0] = [200, 200, 255]  # ì—°í•œ ë¹¨ê°„ìƒ‰
        result = cv2.addWeighted(result, 0.85, furniture_overlay, 0.15, 0)
        
        # 2. AI ê°ì§€ëœ ê°ì²´ë“¤ í‘œì‹œ
        for obj in detected_objects:
            x, y, w, h = obj['bbox']
            obj_type = obj['type']
            confidence = obj['confidence']
            method = obj.get('method', 'unknown')
            
            # ê°ì²´ íƒ€ì…ë³„ ìƒ‰ìƒ
            colors = {
                'door': (0, 255, 0),        # ì´ˆë¡ (ë¬¸)
                'window': (255, 255, 0),    # ë…¸ë‘ (ì°½ë¬¸)
                'furniture': (255, 0, 255), # ë§ˆì  íƒ€ (ê°€êµ¬)
                'outlet': (0, 255, 255)     # ì‹œì•ˆ (ì½˜ì„¼íŠ¸)
            }
            
            color = colors.get(obj_type, (128, 128, 128))
            
            # AI ê°ì§€ëœ ê°ì²´ëŠ” ë‘êº¼ìš´ ì„ ìœ¼ë¡œ í‘œì‹œ
            thickness = 4 if 'ai' in method else 2
            
            # ê°ì²´ ë°•ìŠ¤
            cv2.rectangle(result, (x, y), (x + w, y + h), color, thickness)
            
            # ë¼ë²¨ (AI ë°©ë²• í‘œì‹œ)
            if 'estimated_real_size' in obj:
                real_size = obj['estimated_real_size']
                if obj_type == 'door':
                    label = f"AI Door {real_size['height']}cm"
                elif obj_type == 'window':
                    label = f"AI Window {real_size['width']}cm"
                else:
                    label = f"AI {obj_type} {confidence:.2f}"
            else:
                label = f"AI {obj_type} {confidence:.2f}"
            
            # ë¼ë²¨ ë°°ê²½
            text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            cv2.rectangle(result, (x, y - 30), (x + text_size[0] + 10, y), color, -1)
            cv2.putText(result, label, (x + 5, y - 8),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # 3. ë°© ì „ì²´ ê²½ê³„ì„  ê°•ì¡° í‘œì‹œ
        if len(corners) >= 4:
            pts = np.array(corners, dtype=np.int32)
            
            # ë°© ì˜ì—­ ë°˜íˆ¬ëª… ì˜¤ë²„ë ˆì´
            room_overlay = result.copy()
            cv2.fillPoly(room_overlay, [pts], (0, 255, 0))
            result = cv2.addWeighted(result, 0.8, room_overlay, 0.2, 0)
            
            # ë°© ê²½ê³„ì„  (ë§¤ìš° ë‘êº¼ìš´ ì„ )
            cv2.polylines(result, [pts], True, (0, 255, 0), 6)
            
            # ëª¨ì„œë¦¬ì  (í¬ê³  ëª…í™•í•˜ê²Œ)
            for i, corner in enumerate(corners):
                cv2.circle(result, corner, 15, (255, 0, 0), -1)
                cv2.circle(result, corner, 20, (255, 255, 255), 4)
                cv2.putText(result, str(i+1), 
                           (corner[0] + 25, corner[1] + 25),
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)
        
        # 4. AI ë¶„ì„ ì •ë³´ íŒ¨ë„
        self.draw_ai_info_panel(result, dimensions, detected_objects)
        
        # 5. ì •í™•í•œ ì¹˜ìˆ˜ ë¼ë²¨
        if len(corners) >= 4:
            self.draw_ai_dimension_labels(result, corners, dimensions)
        
        # base64 ì¸ì½”ë”©
        _, buffer = cv2.imencode('.png', result)
        result_base64 = base64.b64encode(buffer).decode('utf-8')
        
        return f"data:image/png;base64,{result_base64}"
    
    def draw_ai_info_panel(self, image, dimensions, detected_objects):
        """AI ì •ë³´ íŒ¨ë„"""
        panel_height = 180
        panel_width = image.shape[1]
        
        panel = np.zeros((panel_height, panel_width, 3), dtype=np.uint8)
        panel.fill(15)
        
        # ì œëª©
        cv2.putText(panel, "AI Enhanced Room Analysis", (20, 25),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)
        
        # ì£¼ìš” ê²°ê³¼
        main_info = [
            f"Room Size: {dimensions['width']}cm x {dimensions['height']}cm",
            f"Area: {dimensions['area']}mÂ² | Confidence: {dimensions['confidence']:.1%}",
            f"AI Objects Detected: {len(detected_objects)}",
        ]
        
        for i, line in enumerate(main_info):
            cv2.putText(panel, line, (20, 55 + i * 25),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # AI ê¸°ëŠ¥ í‘œì‹œ
        ai_features = [
            "âœ“ Wall/Furniture Separation",
            "âœ“ AI Object Detection", 
            "âœ“ Smart Scale Estimation"
        ]
        
        for i, feature in enumerate(ai_features):
            cv2.putText(panel, feature, (20, 125 + i * 18),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        # íŒ¨ë„ ì¶”ê°€
        result = np.vstack([image, panel])
        image[:] = result[:image.shape[0]]
    
    def draw_ai_dimension_labels(self, image, corners, dimensions):
        """AI ì¹˜ìˆ˜ ë¼ë²¨"""
        # ê°€ë¡œ ì¹˜ìˆ˜ (ìƒë‹¨)
        top_center = (
            (corners[0][0] + corners[1][0]) // 2,
            max(50, corners[0][1] - 60)
        )
        
        width_text = f"Room Width: {dimensions['width']}cm"
        self.draw_dimension_label(image, width_text, top_center, (0, 255, 255))
        
        # ì„¸ë¡œ ì¹˜ìˆ˜ (ì¢Œì¸¡)
        left_center = (
            max(150, corners[0][0] - 140),
            (corners[0][1] + corners[3][1]) // 2
        )
        
        height_text = f"Room Height: {dimensions['height']}cm"
        self.draw_dimension_label(image, height_text, left_center, (0, 255, 255))
    
    def draw_dimension_label(self, image, text, position, color):
        """ì¹˜ìˆ˜ ë¼ë²¨ ê·¸ë¦¬ê¸°"""
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1.0
        thickness = 3
        
        text_size = cv2.getTextSize(text, font, font_scale, thickness)[0]
        
        # ë°°ê²½ ë°•ìŠ¤
        cv2.rectangle(image, 
                     (position[0] - text_size[0]//2 - 15, position[1] - text_size[1] - 15),
                     (position[0] + text_size[0]//2 + 15, position[1] + 15),
                     (0, 0, 0), -1)
        
        # í…Œë‘ë¦¬
        cv2.rectangle(image, 
                     (position[0] - text_size[0]//2 - 15, position[1] - text_size[1] - 15),
                     (position[0] + text_size[0]//2 + 15, position[1] + 15),
                     color, 3)
        
        # í…ìŠ¤íŠ¸
        cv2.putText(image, text, 
                   (position[0] - text_size[0]//2, position[1] - 5),
                   font, font_scale, color, thickness)

# Flask ë¼ìš°íŠ¸
analyzer = AIEnhancedRoomAnalyzer()

@app.route('/')
def index():
    return render_template('room_analyzer.html')

@app.route('/api/analyze', methods=['POST'])
def analyze_room():
    try:
        data = request.get_json()
        
        image_data = data.get('image')
        reference_size = data.get('reference_size')
        options = data.get('options', {})
        
        if not image_data:
            return jsonify({'success': False, 'error': 'ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.'})
        
        logger.info("4ë‹¨ê³„ AI ê°•í™” ë¶„ì„ ì‹œì‘")
        result = analyzer.analyze_image(image_data, reference_size, options)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"API ì˜¤ë¥˜: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/health')
def health_check():
    return jsonify({
        'status': 'healthy',
        'version': '4.0.0',
        'features': [
            'ai_structure_analysis',
            'wall_furniture_separation', 
            'ai_object_detection',
            'smart_scale_estimation',
            'full_room_boundary_detection',
            'ai_verification'
        ],
        'timestamp': datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("ğŸš€ 4ë‹¨ê³„ AI ê°•í™” ë°© ë¶„ì„ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“ http://localhost:5000 ì—ì„œ ì ‘ì†í•˜ì„¸ìš”")
    print("ğŸ¤– AI ê°•í™” ê¸°ëŠ¥:")
    print("  ğŸ§  AI êµ¬ì¡° ë¶„ì„ (ë²½ë©´/ê°€êµ¬/ë°”ë‹¥/ì²œì¥ ë¶„ë¦¬)")
    print("  ğŸ¯ ë²½ë©´ ì „ìš© ê²½ê³„ì„  ê°ì§€")
    print("  ğŸ” AI ê°ì²´ ê°ì§€ (Google Vision API ì§€ì›)")
    print("  ğŸ“ ìŠ¤ë§ˆíŠ¸ ìŠ¤ì¼€ì¼ ì¶”ì •")
    print("  âœ… AI ê¸°ë°˜ ê²°ê³¼ ê²€ì¦")
    print("  ğŸ¨ ê³ ê¸‰ ì‹œê°í™” (êµ¬ì¡° ë¶„ì„ ê²°ê³¼ í‘œì‹œ)")
    print("  ğŸ  ë°© ì „ì²´ ì˜ì—­ ì •í™• ì¸¡ì •")
    app.run(debug=True, host='0.0.0.0', port=5000)